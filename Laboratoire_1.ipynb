{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projet analyse des sentiments avec machine learning\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 - Overview\n",
    "Twitter is a mix of social network and microblogging. In this network, people post information and communicate among themselves through messages, called tweets, that can contain up to 280 characters.\n",
    "In this assignment, we will implement a prototype that can detect if an airline company is positively or negatively mentioned in a tweet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hide_input": true
   },
   "source": [
    "## 2.1 -  Setup\n",
    "install the packages needed for this assignment and upload them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you want, you can use anaconda and install after nltk library\n",
    "# pip install --user numpy\n",
    "# pip install --user sklearn\n",
    "# pip install --user scipy\n",
    "# pip install --user nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\salha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\salha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\salha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package universal_tagset to\n",
      "[nltk_data]     C:\\Users\\salha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package universal_tagset is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#python\n",
    "import nltk\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('universal_tagset')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 - Dataset\n",
    "\n",
    "We download the zip file in the following url: https://drive.google.com/file/d/1iGmESwPXpO3sIZFGOCrysxJ27AHdly-Y/view?usp=sharing\n",
    "\n",
    "In this zip file, there are 2 files:\n",
    "1. train.tsv: training dataset\n",
    "2. dev.tsv: validation dataset\n",
    "\n",
    "Each line of the files has the following information about a tweet: *tweet id*, *user id*, *label* and *message text*.\n",
    "\n",
    "There are three labels in the dataset: *negative*, *neutral* and *positive*. We represent each one of these labels as 0, 1 and 2 respectively.\n",
    "\n",
    "In the code above we read the training and validation datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "import re\n",
    "\n",
    "def load_dataset(path):\n",
    "    dtFile = codecs.open(path, 'r')\n",
    "    \n",
    "    x = []\n",
    "    y = []\n",
    "    \n",
    "    for l in dtFile:\n",
    "        sid, uid, label,text = re.split(r\"\\s+\", l, maxsplit=3)\n",
    "        \n",
    "        text = text.strip()\n",
    "        \n",
    "        # Remove not available\n",
    "        if text == \"Not Available\":\n",
    "            continue\n",
    "        \n",
    "        x.append(text)\n",
    "        \n",
    "        if label == \"negative\": \n",
    "            y.append(0)\n",
    "        elif label == \"neutral\": \n",
    "            y.append(1)\n",
    "        elif label == \"positive\": \n",
    "            y.append(2)\n",
    "        \n",
    "    assert len(x) == len(y)\n",
    "            \n",
    "    return x,y\n",
    "            \n",
    "\n",
    "# Path of training dataset\n",
    "trainingPath=\"./train_data.tsv\"\n",
    "\n",
    "# Path of validation dataset\n",
    "validationPath=\"./dev_data.tsv\"\n",
    "\n",
    "training_X, training_Y = load_dataset(trainingPath)\n",
    "validation_X, validation_Y = load_dataset(validationPath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 - Preprocessing\n",
    "\n",
    "Preprocessing is a crucial task in data mining. This task clean and transform the raw data in a format that can better suit data analysis and machine learning techniques. In natural language processing (NLP), *tokenization* and *stemming* are two well known preprocessing steps. Besides these two steps, we will implement an additional step that is designed exclusively for the twitter domain.\n",
    "\n",
    "### 2.3.1 - Tokenization\n",
    "\n",
    "In this preprocessing step, a *tokenizer* is responsible for breaking a text in a sequence of tokens (words, symbols, and punctuations). For instance, the sentence *\"It's the student's notebook.\"* can be split into the following list of tokens: ['It', \"'s\", 'the', 'student', \"'s\", 'notebook', '.'].\n",
    "\n",
    "\n",
    "#### 2.3.1.1  \n",
    "\n",
    "Implementation of the SpaceTokenizer and NLTKTokenizer tokenizers: \n",
    "- **SpaceTokenizer** tokenizes the tokens that are separated by whitespace (space, tab, newline). This is a naive tokenizer.\n",
    "- **NLTKTokenizer** uses the default method of the nltk package (https://www.nltk.org/api/nltk.html) to tokenize the text.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "class SpaceTokenizer(object):\n",
    "    \"\"\"\n",
    "    It tokenizes the tokens that are separated by whitespace (space, tab, newline). \n",
    "    We consider that any tokenization was applied in the text when we use this tokenizer.\n",
    "    \n",
    "    For example: \"hello\\tworld of\\nNLP\" is split in ['hello', 'world', 'of', 'NLP']\n",
    "    \"\"\"\n",
    "    \n",
    "    def tokenize(self, text):\n",
    "\n",
    "        tokens = text.replace('\\n',' ').replace('\\t',' ').split()\n",
    "        \n",
    "        # Have to return a list of tokens\n",
    "        return tokens\n",
    "        \n",
    "class NLTKTokenizer(object):\n",
    "    \"\"\"\n",
    "    This tokenizer uses the default function of nltk package (https://www.nltk.org/api/nltk.html) to tokenize the text.\n",
    "    \"\"\"\n",
    "    \n",
    "    def tokenize(self, text):\n",
    "\n",
    "        tokens = nltk.word_tokenize(text)\n",
    "        \n",
    "        # Have to return a list of tokens\n",
    "        return tokens\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.2 - Stemming\n",
    "\n",
    "In the tweets *\"I should have bought a new shoes today\"* and *\"I spent too much money buying games\"*, the words *\"buy\"* and *\"bought\"* represent basically the same concept. Considering both words as different can unnecessarily increase the dimensionality of the problem and can negatively impact the performance of simple models. Therefore, a unique form (e.g., the root buy) can represent both words. The process to convert words with the same stem (word reduction that keeps word prefixes) to a standard form is called *stemming*.\n",
    "\n",
    "#### 2.3.2.1 \n",
    "\n",
    "Retrieve the stems of the tokens using the attribute *stemmer* from the class *Stemmer*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello', 'fair', 'fair']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "class Stemmer(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.stemmer = SnowballStemmer(\"english\", ignore_stopwords=True)\n",
    "    \n",
    "    def stem(self, tokens):\n",
    "        \"\"\"\n",
    "        tokens: a list of strings\n",
    "        \"\"\"\n",
    "        stemTokens = []\n",
    "        for word in tokens:\n",
    "            stemTokens.append(self.stemmer.stem(word))\n",
    "        # Have to return a list of stems\n",
    "        return stemTokens\n",
    "\n",
    "test = Stemmer()\n",
    "print(test.stem([\"hello\", \"fair\", \"fairly\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.3 - Twitter preprocessing\n",
    "\n",
    "Sometimes only applying the default NLP preprocessing steps is not enough. Data for certain domains can have peculiar characteristics which requires specific preprocessing steps to remove the noise and create a more suitable format for the models. \n",
    "\n",
    "In NLP, methods store a set of words, called dictionary, and all the words out of the dictionary are considered as unknown. In this assignment, the feature space dimensionality of a model is directly related to the number of words in the dictionary. Since high-dimensional spaces can suffer from the curse of dimensionality, our goal is to create preprocessing steps that decrease vocabulary size.  \n",
    "\n",
    "#### 2.3.3.1 \n",
    "\n",
    "implementation of two preprocessing steps that reduce the dictionary size (number of unique words). These preprocessing steps must be related to the specific characteristic of the Twitter data. Therefore, for instance, the stop words removal will not be accepted as a preprocessing step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello it's me ?\n"
     ]
    }
   ],
   "source": [
    "class TwitterPreprocessing(object):\n",
    "    \n",
    "    def preprocess(self, tweet):\n",
    "        \"\"\"\n",
    "        tweet: original tweet\n",
    "        \"\"\"\n",
    "        \n",
    "        # Remove every mentions to another profile\n",
    "        # Indeed, the name of a profile doesn't give much information and just increases the dictionary size\n",
    "        # On Twitter, a mention always begin by \"@\"\n",
    "        tweet = ' '.join(word for word in tweet.split() if word[0]!='@')\n",
    "        \n",
    "        # Remove every tags\n",
    "        # Indeed, most tags are created by the users and are an aggregation of several words\n",
    "        # Eliminate them reduce the dictionary size but a better way would be to decompose the words if there are\n",
    "        # On Twitter, a tag always begin by \"#\"\n",
    "        tweet = ' '.join(word for word in tweet.split() if word[0]!='#')\n",
    "        \n",
    "        # return the preprocessed twitter\n",
    "        return tweet\n",
    "\n",
    "test = TwitterPreprocessing()\n",
    "print(test.preprocess(\"Hello @jo it's me ? #funnyhere\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.3  Pipeline\n",
    "\n",
    "The pipeline is sequence of preprocessing steps that transform the raw data to a format that is suitable for your problem. We implement the class *PreprocessingPipeline* that apply the tokenizer, twitter preprocessing and stemer to the text.\n",
    "\n",
    "**Feel free to change the preprocessing order.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreprocessingPipeline:\n",
    "    \n",
    "    def __init__(self, tokenization, twitterPreprocessing, stemming):\n",
    "        \"\"\"\n",
    "        tokenization: enable or disable tokenization.\n",
    "        twitterPreprocessing: enable or disable twitter preprocessing.\n",
    "        stemming: enable or disable stemming.\n",
    "        \"\"\"\n",
    "\n",
    "        self.tokenizer= NLTKTokenizer() if tokenization else SpaceTokenizer()\n",
    "        self.twitterPreprocesser = TwitterPreprocessing() if twitterPreprocessing else None\n",
    "        self.stemmer = Stemmer() if stemming else None\n",
    "    \n",
    "    def preprocess(self, tweet):\n",
    "        \"\"\"\n",
    "        Transform the raw data\n",
    "\n",
    "        tokenization: boolean value.\n",
    "        twitterPreprocessing: boolean value. Apply the\n",
    "        stemming: boolean value.\n",
    "        \"\"\"\n",
    "        if self.twitterPreprocesser:\n",
    "            tweet = self.twitterPreprocesser.preprocess(tweet)\n",
    "        \n",
    "        tokens = self.tokenizer.tokenize(tweet)\n",
    "\n",
    "        if self.stemmer:\n",
    "            tokens = self.stemmer.stem(tokens)\n",
    "            \n",
    "        return tokens\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 N-grams\n",
    "\n",
    "An n-gram is a contiguous sequence of *n* tokens from a text. Thus, for instance,the sequence *\"bye as\"* and *\"walked through\"* are example of 2-grams from the sentence *\"He said bye as he walked through the door .\"*. 1-gram, 2-gram and 3-gram are, respectively, called unigram, bigram and trigram. We list all the possible unigram, bigram and trigram from the *\"He said bye as he walked through the door .\"*:\n",
    "\n",
    "- Unigram: [\"He\", \"said\", \"bye\", \"as\", \"he\", \"walked\", \"through\", \"the\", \"door\", \".\"]\n",
    "- Bigram: [\"He said\", \"said bye\", \"bye as\", \"as he\", \"he walked\", \"walked through\", \"through the\", \"the door\", \"door .\"] \n",
    "- Trigram: [\"He said bye\", \"said bye as\", \"bye as he\", \"as he walked\", \"he walked through\", \"walked through the\", \"through the door\", \"the door .\"] \n",
    "\n",
    "\n",
    "### 2.4.1 \n",
    "\n",
    "Implement bigram and trigram.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['He said', 'said bye', 'bye as', 'as he', 'he walked', 'walked through', 'through the', 'the door', 'door .']\n",
      "['He said bye', 'said bye as', 'bye as he', 'as he walked', 'he walked through', 'walked through the', 'through the door', 'the door .']\n"
     ]
    }
   ],
   "source": [
    "def bigram(tokens):\n",
    "    \"\"\"\n",
    "    tokens: a list of strings\n",
    "    \"\"\"\n",
    "    bigrams = []\n",
    "    \n",
    "    for i in range(len(tokens)-1):\n",
    "        bigrams.append(tokens[i] + \" \" + tokens[i+1])\n",
    "        \n",
    "    return bigrams\n",
    "\n",
    "def trigram(tokens):\n",
    "    \"\"\"\n",
    "    tokens: a list of strings\n",
    "    \"\"\"\n",
    "    trigrams = []\n",
    "    \n",
    "    for i in range(len(tokens)-2):\n",
    "        trigrams.append(tokens[i] + \" \" + tokens[i+1] + \" \" + tokens[i+2])\n",
    "        \n",
    "    return trigrams\n",
    "    \n",
    "test = [\"He\", \"said\", \"bye\", \"as\", \"he\", \"walked\", \"through\", \"the\", \"door\", \".\"]\n",
    "print(bigram(test))\n",
    "print(trigram(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 Bag-of-words\n",
    "\n",
    "Logistic regression, SVM and other well-known models only accept inputs that have the same size. However, there are some data types whose sizes are not fixed, for instance, a text can have an unlimited number of words. Imagine that we retrieve two tweets: ”Board games are much better than video games” and ”Pandemic is an awesome game!”. These sentences are respectively named as Sentence 1 and 2. Table below depicts how we could represent both sentences using a fixed representation.\n",
    "\n",
    "|            | an | are | ! | pandemic | awesome | better | games | than | video | much | board | is | game |\n",
    "|------------|----|-----|---|----------|---------|--------|-------|------|-------|------|-------|----|------|\n",
    "| Sentence 1 | 0  | 1   | 0 | 0        | 0       | 1      | 2     | 1    | 1     | 1    | 1     | 0  | 0    |\n",
    "| Sentence 2 | 1  | 0   | 0 | 1        | 1       | 0      | 0     | 0    | 0     | 0    | 0     | 1  | 1    |\n",
    "\n",
    "Each column of this table 2.1 represents one of 13 vocabulary words, whereas the rows contains the word\n",
    "frequencies in each sentence. For instance, the cell in row 1 and column 7 has the value 2\n",
    "because the word games occurs twice in Sentence 1. Since the rows have always 13 values, we\n",
    "could use those vectors to represent the Sentences 1 and 2. The table above illustrates a technique called bag-of-words. Bag-of-words represents a document as a vector whose dimensions are equal to the number of times that vocabulary words appeared in the document. Thus, each token will be related to a dimension, i.e., an integer.\n",
    "\n",
    "### 2.5.1\n",
    "\n",
    "Implement the bag-of-words model that weights the vector with the absolute word frequency.\n",
    "\n",
    "**For this exercise, you cannot use any external python library (e.g., scikit-learn). However, if you have a problem with memory size, you can use the class scipy.sparse.csr_matrix (https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csc_matrix.html)\n",
    "**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "class CountBoW(object):\n",
    "    \n",
    "    def __init__(self, pipeline, bigram=False, trigram=False, words_indexed={}):\n",
    "        \"\"\"\n",
    "        pipelineObj: instance of PreprocesingPipeline\n",
    "        bigram: enable or disable bigram\n",
    "        trigram: enable or disable trigram\n",
    "        \"\"\"\n",
    "        self.pipeline = pipeline\n",
    "        self.bigram = bigram\n",
    "        self.trigram = trigram\n",
    "        self.words_indexed = words_indexed\n",
    "        \n",
    "    def fit_transform(self, X):\n",
    "        \"\"\"\n",
    "        This method preprocesses the data using the pipeline object, relates each unigram, bigram or trigram \n",
    "        to a specific integer and transforms the text in a vector. \n",
    "        Vectors are weighted using the token frequencies in the sentence.\n",
    "        \n",
    "        X: a list that contains tweet contents\n",
    "        \n",
    "        :return: a list of vectors\n",
    "        \"\"\"\n",
    "        \n",
    "        # A list of lists of tokens of each tweet\n",
    "        tweets_tokenized = []\n",
    "        \n",
    "        for tweet in X:\n",
    "            # Preprocess of data\n",
    "            tokens = self.pipeline.preprocess(tweet)\n",
    "            \n",
    "            # List of tokens of the current tweet\n",
    "            all_tokens = []\n",
    "            \n",
    "            for token in tokens:\n",
    "                all_tokens.append(token)\n",
    "            \n",
    "            if self.bigram:\n",
    "                # Adding bigrams\n",
    "                bigrams = bigram(tokens)\n",
    "                for token in bigrams:\n",
    "                    all_tokens.append(token)\n",
    "                    \n",
    "            if self.trigram:\n",
    "                # Adding trigrams\n",
    "                trigrams = trigram(tokens)\n",
    "                for token in trigrams:\n",
    "                    all_tokens.append(token)\n",
    "                    \n",
    "            tweets_tokenized.append(all_tokens)\n",
    "        \n",
    "        # The vocabulary is put in a set\n",
    "        set_all_words = set()\n",
    "        for tweet in tweets_tokenized:\n",
    "            for word in tweet:\n",
    "                set_all_words.add(word)\n",
    "        \n",
    "        # Each word of the vocabulary is associated to an integer\n",
    "        all_words = list(set_all_words)\n",
    "        words_indexed = {}\n",
    "        i = 0\n",
    "        for word in all_words:\n",
    "            words_indexed[word] = i\n",
    "            i += 1\n",
    "        \n",
    "        # The vocabulary-integer associations are saved\n",
    "        self.words_indexed = words_indexed\n",
    "        \n",
    "        # Every tweet is converted to bag-of-words vector\n",
    "        indptr = [0]\n",
    "        indices = []\n",
    "        data = []\n",
    "\n",
    "        for tweet in tweets_tokenized:\n",
    "            for token in tweet:\n",
    "                index = words_indexed[token]\n",
    "                indices.append(index)\n",
    "                data.append(1)\n",
    "            indptr.append(len(indices))\n",
    "        \n",
    "        bow = csr_matrix((data, indices, indptr), dtype=int).toarray()\n",
    "        \n",
    "        return bow\n",
    "        \n",
    "    def transform(self, X):\n",
    "        \"\"\"\n",
    "        This method preprocesses the data using the pipeline object and  transforms the text in a list of integer.\n",
    "        Vectors are weighted using the token frequencies in the sentence.\n",
    "        \n",
    "        X: a list of vectors\n",
    "        \n",
    "        :return: a list of vectors\n",
    "        \"\"\"\n",
    "        \n",
    "        # A list of lists of tokens of each tweet\n",
    "        tweets_tokenized = []\n",
    "        \n",
    "        for tweet in X:\n",
    "            # Preprocess of data\n",
    "            tokens = self.pipeline.preprocess(tweet)\n",
    "            \n",
    "            # List of tokens of the current tweet\n",
    "            all_tokens = []\n",
    "            \n",
    "            for token in tokens:\n",
    "                all_tokens.append(token)\n",
    "            \n",
    "            if self.bigram:\n",
    "                # Adding bigrams\n",
    "                bigrams = bigram(tokens)\n",
    "                for token in bigrams:\n",
    "                    all_tokens.append(token)\n",
    "                    \n",
    "            if self.trigram:\n",
    "                # Adding trigrams\n",
    "                trigrams = trigram(tokens)\n",
    "                for token in trigrams:\n",
    "                    all_tokens.append(token)\n",
    "                    \n",
    "            tweets_tokenized.append(all_tokens)\n",
    "        \n",
    "        indptr = [0]\n",
    "        indices = []\n",
    "        data = []\n",
    "\n",
    "        for tweet in tweets_tokenized:\n",
    "            for token in tweet:\n",
    "                if token in self.words_indexed:\n",
    "                    index = self.words_indexed[token]\n",
    "                    indices.append(index)\n",
    "                    data.append(1)\n",
    "            indptr.append(len(indices))\n",
    "        \n",
    "        bow = csr_matrix((data, indices, indptr), dtype=int, shape=(len(tweets_tokenized), len(self.words_indexed))).toarray()\n",
    "        \n",
    "        return bow\n",
    "      \n",
    "        \n",
    "# testSentences = [\"Board games are much better than video games\", \"Pandemic is an awesome game!\"]\n",
    "# test = CountBoW(PreprocessingPipeline(True, True, True))\n",
    "# validation = [\"I love this board game!\"]\n",
    "# test1 = test.fit_transform(testSentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5.2 - TF-IDF\n",
    "\n",
    "Using raw frequency in the bag-of-words can be problematic. The word frequency distribution\n",
    "is skewed - only a few words have high frequencies in a document. Consequently, the\n",
    "weight of these words will be much bigger than the other ones which can give them more\n",
    "impact on some tasks, like similarity comparison. Besides that, a set of words (including\n",
    "those with high frequency) appears in most of the documents and, therefore, they do not\n",
    "help to discriminate documents. For instance, the word *of* appears in a significant\n",
    "part of tweets. Thus, having the word *of* does not make\n",
    "documents more or less similar. However, the word *terrible* is rarer and documents that\n",
    "have this word are more likely to be negative. TF-IDF is a technique that overcomes the word frequency disadvantages.\n",
    "\n",
    "TF-IDF weights the vector using inverse document frequency (IDF) and word frequency, called term frequency (TF).\n",
    "TF is the local information about how important is a word to a specific document.  IDF measures the discrimination level of the words in a dataset.  Common words in a domain are not helpful to discriminate documents since most of them contain these terms. So, to reduce their relevance in the documents, these words should have low weights in the vectors . \n",
    "The following equation calculates the word IDF:\n",
    "\\begin{equation}\n",
    "\tidf_i = \\log\\left( \\frac{N}{df_i} \\right),\n",
    "\\end{equation}\n",
    "where $N$ is the number of documents in the dataset, $df_i$ is the number of documents that contain a word $i$.\n",
    "The new weight $w_{ij}$ of a word $i$ in a document $j$ using TF-IDF is computed as:\n",
    "\\begin{equation}\n",
    "\tw_{ij} = tf_{ij} \\times idf_i,\n",
    "\\end{equation}\n",
    "where $tf_{ij}$ is the term frequency of word $i$ in the document $j$.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### 2.5.2.1\n",
    "\n",
    "Implement a bag-of-words model that weights the vector using TF-IDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "class TFIDFBoW(object):\n",
    "    \n",
    "    def __init__(self, pipeline, bigram=False, trigram=False, words_indexed={}, IDF=[]):\n",
    "        \"\"\"\n",
    "        pipelineObj: instance of PreprocesingPipeline\n",
    "        bigram: enable or disable bigram\n",
    "        trigram: enable or disable trigram\n",
    "        \"\"\"\n",
    "        self.pipeline = pipeline\n",
    "        self.bigram = bigram\n",
    "        self.trigram = trigram\n",
    "        self.words_indexed = words_indexed\n",
    "        self.IDF = IDF\n",
    "\n",
    "        \n",
    "    def fit_transform(self, X):\n",
    "        \"\"\"\n",
    "        This method preprocesses the data using the pipeline object, calculates the IDF and TF and \n",
    "        transforms the text in vectors. Vectors are weighted using TF-IDF method.\n",
    "        \n",
    "        X: a list that contains tweet contents\n",
    "        \n",
    "        :return: a list that contains the list of integers\n",
    "        \"\"\"\n",
    "        \n",
    "        # A list of lists of tokens of each tweet\n",
    "        tweets_tokenized = []\n",
    "        \n",
    "        for tweet in X:\n",
    "            # Preprocess of data\n",
    "            tokens = self.pipeline.preprocess(tweet)\n",
    "            \n",
    "            # List of tokens of the current tweet\n",
    "            all_tokens = []\n",
    "            \n",
    "            for token in tokens:\n",
    "                all_tokens.append(token)\n",
    "            \n",
    "            if self.bigram:\n",
    "                # Adding bigrams\n",
    "                bigrams = bigram(tokens)\n",
    "                for token in bigrams:\n",
    "                    all_tokens.append(token)\n",
    "                    \n",
    "            if self.trigram:\n",
    "                # Adding trigrams\n",
    "                trigrams = trigram(tokens)\n",
    "                for token in trigrams:\n",
    "                    all_tokens.append(token)\n",
    "                    \n",
    "            tweets_tokenized.append(all_tokens)\n",
    "        \n",
    "        # The vocabulary in a set\n",
    "        set_all_words = set()\n",
    "        for tweet in tweets_tokenized:\n",
    "            for word in tweet:\n",
    "                set_all_words.add(word)\n",
    "        \n",
    "        # Each word of the vocabulary is associated to an integer\n",
    "        all_words = list(set_all_words)\n",
    "        words_indexed = {}\n",
    "        i = 0\n",
    "        for word in all_words:\n",
    "            words_indexed[word] = i\n",
    "            i += 1\n",
    "        \n",
    "        # The vocabulary-integer associations are saved\n",
    "        self.words_indexed = words_indexed\n",
    "        \n",
    "        # Every tweet is converted to bag-of-words vector\n",
    "        indptr = [0]\n",
    "        indices = []\n",
    "        data = []\n",
    "\n",
    "        for tweet in tweets_tokenized:\n",
    "            for token in tweet:\n",
    "                index = words_indexed[token]\n",
    "                indices.append(index)\n",
    "                data.append(1)\n",
    "            indptr.append(len(indices))\n",
    "        \n",
    "        bow = csr_matrix((data, indices, indptr), dtype=int).toarray()\n",
    "        \n",
    "        # The number of tweets\n",
    "        N = len(tweets_tokenized)\n",
    "        \n",
    "        # For each word, the number of tweets cotaining it\n",
    "        IDF = []\n",
    "        for i in range(len(words_indexed)):\n",
    "            count = 0\n",
    "            for j in range(N):\n",
    "            #for bag in bow:\n",
    "                #if bag.toarray()[0][i] > 0:\n",
    "                if bow[j][i] > 0:\n",
    "                    count += 1\n",
    "            IDF.append(math.log(N/count))\n",
    "        \n",
    "        # The IDF for each word is saved\n",
    "        self.IDF = IDF\n",
    "        \n",
    "        indptr = [0]\n",
    "        indices = []\n",
    "        data = []\n",
    "\n",
    "        for bag in bow:\n",
    "            for index, count in enumerate(bag):\n",
    "                if count != 0 and IDF[index] != 0:\n",
    "                    indices.append(index)\n",
    "                    data.append(bag[index] * IDF[index])\n",
    "            indptr.append(len(indices))\n",
    "\n",
    "        bowTFIDF = csr_matrix((data, indices, indptr), dtype=float)#.toarray()\n",
    "        print(\"##4\")\n",
    "        return bowTFIDF\n",
    "        \n",
    "    def transform(self, X):\n",
    "        \"\"\"\n",
    "        This method preprocesses the data using the pipeline object and  \n",
    "            transforms the text in a list of integer.\n",
    "        \n",
    "        X: a list of vectors\n",
    "        \n",
    "        :return: a list of vectors\n",
    "        \"\"\"        \n",
    "        \n",
    "        # A list of lists of tokens of each tweet\n",
    "        tweets_tokenized = []\n",
    "        \n",
    "        for tweet in X:\n",
    "            # Preprocess of data\n",
    "            tokens = self.pipeline.preprocess(tweet)\n",
    "            \n",
    "            # List of tokens of the current tweet\n",
    "            all_tokens = []\n",
    "            \n",
    "            for token in tokens:\n",
    "                all_tokens.append(token)\n",
    "            \n",
    "            if self.bigram:\n",
    "                # Adding bigrams\n",
    "                bigrams = bigram(tokens)\n",
    "                for token in bigrams:\n",
    "                    all_tokens.append(token)\n",
    "                    \n",
    "            if self.trigram:\n",
    "                # Adding trigrams\n",
    "                trigrams = trigram(tokens)\n",
    "                for token in trigrams:\n",
    "                    all_tokens.append(token)\n",
    "                    \n",
    "            tweets_tokenized.append(all_tokens)\n",
    "\n",
    "        indptr = [0]\n",
    "        indices = []\n",
    "        data = []\n",
    "\n",
    "        for tweet in tweets_tokenized:\n",
    "            for token in tweet:\n",
    "                if token in self.words_indexed:\n",
    "                    index = self.words_indexed[token]\n",
    "                    indices.append(index)\n",
    "                    data.append(1)\n",
    "            indptr.append(len(indices))\n",
    "\n",
    "        bow = csr_matrix((data, indices, indptr), dtype=int, shape=(len(tweets_tokenized), len(self.words_indexed))).toarray()\n",
    "\n",
    "        indptr = [0]\n",
    "        indices = []\n",
    "        data = []\n",
    "        \n",
    "        for bag in bow:\n",
    "            for index, count in enumerate(bag):\n",
    "                if count != 0 and self.IDF[index] != 0:\n",
    "                    indices.append(index)\n",
    "                    data.append(bag[index] * self.IDF[index])\n",
    "            indptr.append(len(indices))\n",
    "        \n",
    "        bowTFIDF = csr_matrix((data, indices, indptr), dtype=float, shape=(len(tweets_tokenized), len(self.words_indexed)))#.toarray()\n",
    "        \n",
    "        return bowTFIDF\n",
    "\n",
    "# testSentences = [\"Board games are much better than video games\", \"Pandemic is an awesome game!\"]\n",
    "# test = TFIDFBoW(PreprocessingPipeline(True, True, True))\n",
    "# validation = [\"I love this board game!\"]\n",
    "# print(test.fit_transform(testSentences))\n",
    "# print(test.transform(validation))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.6 - Classifier using BoW\n",
    "\n",
    "We are going to use logistic regression as a classifier. Read the following page to now more about this classifier: https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n",
    "\n",
    "\n",
    "The method *train_evaluate* trains and evaluates the logistic regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "def train_evaluate(training_X, training_Y, validation_X, validation_Y, bowObj):\n",
    "    \"\"\"\n",
    "    training_X: tweets from the training dataset\n",
    "    training_Y: tweet labels from the training dataset\n",
    "    validation_X: tweets from the validation dataset\n",
    "    validation_Y: tweet labels from the validation dataset\n",
    "    bowObj: Bag-of-word object\n",
    "    \n",
    "    :return: the classifier and its accuracy in the training and validation dataset.\n",
    "    \"\"\"\n",
    "    \n",
    "    classifier = LogisticRegression()\n",
    "    \n",
    "    training_rep = bowObj.fit_transform(training_X)\n",
    "    #dicSize = len(training_rep[0])\n",
    "    dicSize = training_rep.get_shape()[1]\n",
    "    \n",
    "    classifier.fit(training_rep, training_Y)\n",
    "   \n",
    "    trainAcc = accuracy_score(training_Y,classifier.predict(training_rep))\n",
    "    validationAcc = accuracy_score(validation_Y,classifier.predict(bowObj.transform(validation_X)))\n",
    "    \n",
    "    return classifier, trainAcc, validationAcc, dicSize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 2.6.1 \n",
    "\n",
    "Train and calculate the logistic regression accuracy in the *training and validation dataset* using each one of the following configurations:\n",
    "    1. CountBoW + SpaceTokenizer(without tokenizer) + unigram \n",
    "    2. CountBoW + NLTKTokenizer + unigram\n",
    "    3. TFIDFBoW + NLTKTokenizer + unigram\n",
    "    3. TFIDFBoW + NLTKTokenizer + Stemming + unigram\n",
    "    4. TFIDFBoW + NLTKTokenizer + Twitter preprocessing + Stemming  + unigram\n",
    "    5. TFIDFBoW + NLTKTokenizer + Twitter preprocessing + Stemming  + unigram + bigram\n",
    "    6. TFIDFBoW + NLTKTokenizer + Twitter preprocessing + Stemming  + unigram + bigram + trigram\n",
    "Finally, describe the results found and answer the following questions:\n",
    "- Which preprocessing has helped the model? Why?\n",
    "- TF-IDF has achieved a better performance than CountBoW? If yes, why do you think that this has occurred? \n",
    "- Has the bigram and trigram improved the performance? If yes, can you mention the reasons of this improvement?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "test1 = train_evaluate(training_X, training_Y, validation_X, validation_Y, CountBoW(PreprocessingPipeline(False, False, False)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Programmes\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\Programmes\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:459: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "test2 = train_evaluate(training_X, training_Y, validation_X, validation_Y, CountBoW(PreprocessingPipeline(True, False, False)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##1\n",
      "##2\n",
      "##3\n",
      "##4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Programmes\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\Programmes\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:459: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "test3 = train_evaluate(training_X, training_Y, validation_X, validation_Y, TFIDFBoW(PreprocessingPipeline(True, False, False)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##1\n",
      "##2\n",
      "##3\n",
      "##4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Programmes\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\Programmes\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:459: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "test4 = train_evaluate(training_X, training_Y, validation_X, validation_Y, TFIDFBoW(PreprocessingPipeline(True, False, True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##1\n",
      "##2\n",
      "##3\n",
      "##4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Programmes\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\Programmes\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:459: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "test5 = train_evaluate(training_X, training_Y, validation_X, validation_Y, TFIDFBoW(PreprocessingPipeline(True, True, True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##1\n",
      "##2\n",
      "##3\n",
      "##4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Programmes\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\Programmes\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:459: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "test6 = train_evaluate(training_X, training_Y, validation_X, validation_Y, TFIDFBoW(PreprocessingPipeline(True, True, True), True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##1\n",
      "##2\n",
      "##3\n",
      "##4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Programmes\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\Programmes\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:459: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "test7 = train_evaluate(training_X, training_Y, validation_X, validation_Y, TFIDFBoW(PreprocessingPipeline(True, True, True), True, True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9950124688279302, 0.9900249376558603, 0.9992665395335192, 0.9992665395335192, 0.9988264632536307, 0.9992665395335192, 0.9992665395335192]\n",
      "[0.5948808473080318, 0.6275375110326566, 0.6257722859664607, 0.6363636363636364, 0.6125330979699912, 0.6328331862312445, 0.6204766107678729]\n",
      "[32157, 24141, 24141, 17677, 14197, 90673, 208662]\n"
     ]
    }
   ],
   "source": [
    "trainAccs = [test1[1], test2[1], test3[1], test4[1], test5[1], test6[1], test7[1]]\n",
    "validationAccs = [test1[2], test2[2], test3[2], test4[2], test5[2], test6[2], test7[2]]\n",
    "dicSizes = [test1[3], test2[3], test3[3], test4[3], test5[3], test6[3], test7[3]]\n",
    "print(trainAccs)\n",
    "print(validationAccs)\n",
    "print(dicSizes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training Accuracies** : \n",
    "\n",
    "[0.9950124688279302, 0.9900249376558603, 0.9992665395335192, 0.9992665395335192, 0.9988264632536307, 0.9992665395335192, 0.9992665395335192]\n",
    "\n",
    "**Validation Accuracies** :\n",
    "\n",
    "[0.5948808473080318, 0.6275375110326566, 0.6257722859664607, 0.6363636363636364, 0.6125330979699912, 0.6328331862312445, 0.6204766107678729]\n",
    "\n",
    "**Dictionary Sizes** : \n",
    "\n",
    "[32157, 24141, 24141, 17677, 14197, 90673, 208662]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The best preprocessing methods are n°4 and n°7** :\n",
    "\n",
    "-> TFIDFBoW + NLTKTokenizer + Stemming + unigram\n",
    "\n",
    "-> TFIDFBoW + NLTKTokenizer + Twitter preprocessing + Stemming  + unigram + bigram\n",
    "\n",
    "This can be explained by the use of TF-IDF, bigrams and stemming.\n",
    "\n",
    "**Stemming** allows to reduce the size of the vocabulary and to associate several different words which have the same meaning.\n",
    "\n",
    "**TF-IDF can achieve a better performance than CountBoW**. Indeed, CountBoW only gives a vector with the number of occurences of each word. This way of counting can lead to some issues. For example, there are some words which are common to every piece of text as \"the\", \"a\", \"is\"... The number of occurences of these one will be high while they don't give us any clue regarding the sentiment expressed. \"Awful\" is much more interesting but less frequent and therefore has a lower weight in CountBoW. TF-IDF performs better by facing those issues. As the words like \"the\" and \"a\" appear in most of the tweets, thet will have low weights while \"awful\" is rarer and will be considered as more important.\n",
    "\n",
    "**Using n-grams can improve performance**. Unigrams are words alone but many times a unique word doesn't convey all the information and we use 2 or 3 words to get a complete sense. In our case of sentiment analysis, \"not like\" is negative but using unigrams will lead us to take into account only \"not\" and \"like\" separately. The same happens for locutions and expressions of 2/3 words or more. That's why by using bigrams and trigrams, it performs better : it can get a more global sense of the sentiment expressed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "bestBowObj = TFIDFBoW(PreprocessingPipeline(True, False, True))\n",
    "bestClassifier = LogisticRegression()\n",
    "training_rep = bestBowObj.fit_transform(training_X)\n",
    "bestClassifier.fit(training_rep, training_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Prototype \n",
    "\n",
    "During the last years, *E Corp* has collected tweets to create a dataset to their sentiment analysis tool. Now, airline companies have contracted *E Corp* to analyze the consumer opinion about them. Your job is to extract information from the tweet database about the following companies: Air France, American, British Airways,  Delta, Southwest, United, Us Airways and Virgin America.\n",
    "\n",
    "*For the prototype, you have to use the best model found in the Section 2.*\n",
    "\n",
    "## 3.1 Dataset\n",
    "\n",
    "In https://drive.google.com/file/d/1Cuw6Y12Bj91vF_iH49mqPZZfJkY92iBY/view?usp=sharing, you can find the raw tweet retrieved by E corp.  Each tweet is represented as json that the have attributes listed in the page https://developer.twitter.com/en/docs/tweets/data-dictionary/overview/tweet-object.\n",
    "\n",
    "** You will answer the question of this section using this tweet database (https://drive.google.com/file/d/1Cuw6Y12Bj91vF_iH49mqPZZfJkY92iBY/view?usp=sharing).**\n",
    "\n",
    "## 3.2 Sentiment Analysis\n",
    "\n",
    "\n",
    "### 3.2.1 \n",
    "\n",
    "Implement the method *extract_tweet_content* that extracts the content of each tweet in the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def extract_tweet_content(raw_tweet_file):\n",
    "    \"\"\"\n",
    "    Extract the tweet content for each json object\n",
    "    \n",
    "    raw_tweet_file: file path that contains all json objects\n",
    "    \n",
    "    :return: a list with the tweet contents\n",
    "    \"\"\"\n",
    "    tweets = []\n",
    "    \n",
    "    with open(raw_tweet_file) as f:\n",
    "        for line in f:\n",
    "            tweets.append(json.loads(line)[\"text\"])\n",
    "    \n",
    "    return tweets\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.1 \n",
    "\n",
    "Implement the method *detect_airline* that detects the airline companies in a tweet. Besides that, explain your approach to detect the companies and its possible drawbacks.\n",
    "\n",
    "The detect_airline has to be able to return if none or more than one airline companies are mentioned in a tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Some explanations**\n",
    "\n",
    "A simple method is used to detect an airline company in a tweet.\n",
    "All the names are stored in a list and for each name in the list, we check the tweet which is a string, contains the name as a substring (case insensitive).\n",
    "One drawback of this method is the probability of false positive especially with words in the names of companies that can be used in other contexts.\n",
    "False negative can also be a problem if acronyms are often used or if people talk about a company without the last word \"airlines\". For example, \"I fly with united\" will give a false negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_airline(tweet):\n",
    "    \"\"\"\n",
    "    Detect and return the airline companies mentioned in the tweet\n",
    "    \n",
    "    tweet: represents the tweet message. Type : string\n",
    "    \n",
    "    :return: list of detected airline companies\n",
    "    \"\"\"\n",
    "    airline_companies = [\"air france\", \"american airlines\", \"british airways\", \"delta airlines\", \"southwest airlines\", \"united airlines\", \"us airways\", \"virgin america\"]\n",
    "    \n",
    "    detected = []\n",
    "    \n",
    "    # Not sensitive to uppercase/lowercase\n",
    "    tweet = tweet.lower()\n",
    "    \n",
    "    for company in airline_companies:\n",
    "        if company in tweet:\n",
    "            detected.append(company)\n",
    "    \n",
    "    # If any company is detected, it will return an empty list\n",
    "    return detected\n",
    "\n",
    "# test = \"Air france is better than delta\"\n",
    "# print(detect_airline(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 3.2.1\n",
    "\n",
    "Implement the method *extract_sentiment* that receives a tweet and extracts its sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_sentiment(classifier, tweet, bowObj):\n",
    "    \"\"\"\n",
    "    Extract the tweet sentiment\n",
    "    \n",
    "    classifier: classifier object\n",
    "    tweet: represents the tweet message. Type : string\n",
    "    \n",
    "    :return: list of detected airline companies\n",
    "    \"\"\"\n",
    "    \n",
    "    return classifier.predict(bowObj.transform([tweet]))[0]\n",
    "\n",
    "# test = \"It's so bad!\"\n",
    "# extract_sentiment(bestClassifier, test, bestBowObj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.1 \n",
    "\n",
    "Using the *extract_tweet_content*, *detect_airline* and *extract_sentiment*, implement a code that generates a bar chart that contains the number of positive, neutral and negatives tweets for each one of the companies. Briefly describe your bar chart (e.g, which was the company with most negative tweets) and how this chart can help airline companies.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = extract_tweet_content(\"e_corp_dataset.txt\")\n",
    "\n",
    "airline_companies = [\"air france\", \"american airlines\", \"british airways\", \"delta airlines\", \"southwest airlines\", \"united airlines\", \"us airways\", \"virgin america\"]\n",
    "    \n",
    "# Each comany is associated to a triplet counting the number of negative, neutral and positive tweets\n",
    "counts = {}\n",
    "for company in airline_companies:\n",
    "    counts[company] = [0, 0, 0]\n",
    "    \n",
    "for tweet in tweets:\n",
    "    \n",
    "    airlines = detect_airline(tweet)\n",
    "    \n",
    "    # The sentiment is extracted only if airlines are detected\n",
    "    if len(airlines) > 0:\n",
    "        sentiment = extract_sentiment(bestClassifier, tweet, bestBowObj)\n",
    "        for company in airlines:\n",
    "            counts[company][sentiment] += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# counts = {'air france': [143, 507, 177], 'american': [724, 2396, 1582], 'british airways': [0, 4, 2], 'delta': [51, 141, 149], 'southwest': [284, 1414, 882], 'united': [642, 2463, 1324], 'us airways': [9, 31, 25], 'virgin america': [1, 9, 7]}\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "X = []\n",
    "negative = []\n",
    "neutral = []\n",
    "positive = []\n",
    "\n",
    "for key, value in counts.items():\n",
    "    X.append(key)\n",
    "    negative.append(value[0])\n",
    "    neutral.append(value[1])\n",
    "    positive.append(value[2])\n",
    "\n",
    "# Constructing the bar chart composed of 3 bars (negative, neutral and positive) for each company\n",
    "df = pd.DataFrame(np.c_[negative, neutral, positive], index=X)\n",
    "df.plot.bar(color = [\"red\", \"grey\", \"green\"])\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DESCRIPTION DU BAR CHART!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 - Term Analysis\n",
    "\n",
    "POS-tagging consists of extracting the part-of-speech (POS) of each token in a sentence. For instance, the table below depicts the part-of-speechs of the sentence *The cat is white!* are.\n",
    "\n",
    "\n",
    "\n",
    "|   The   | cat  |  is  | white     |    !       |\n",
    "|---------|------|------|-----------|------------|\n",
    "| article | noun | verb | adjective | punctation |\n",
    "\n",
    "\n",
    "The part-of-speech can be more complex than what we have learned in the school. Linguistics need to have a more detailed information about systax information of the words in a sentence. For our problem, we do not need this level of information and, thus, we will use a less complex set, called universal POS tags. \n",
    "\n",
    "In POS-tagging, each part-of-speech is represented by a tag. You can find the POS tag list used in this assignement at https://universaldependencies.org/u/pos/ ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The', 'DET'), ('cat', 'NOUN'), ('is', 'VERB'), ('white', 'ADJ'), ('!', '.')]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NLTK POS-tagger\n",
    "\n",
    "import nltk\n",
    "\n",
    "\n",
    "#before using pos_tag function, you have to tokenize the sentence.\n",
    "s = ['The', 'cat', 'is',  'white', '!']\n",
    "nltk.pos_tag(s,tagset='universal')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.1 \n",
    "\n",
    "**Implement a code** that retrieves the top 10 most frequent terms for each airline company. You will only consider the terms that appear in a positive and negative tweets. Besides that, we consider as term:\n",
    "1. Words that are either an adjective or a noun\n",
    "2. n-grams that are composed by adjectives followed by a noun (e.g., dirty place) or a noun followed by another noun (e.g.,sports club).\n",
    "\n",
    "Moreover, **generate a table** with the top 10 most frequent terms and their normalized frequencies(percentage) for each airline company.\n",
    "\n",
    "**Do not forget to remove the company names from the chart.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting the tweets\n",
    "#tweets = extract_tweet_content(\"e_corp_dataset.txt\")\n",
    "#tweets = extract_tweet_content(\"test.txt\")\n",
    "\n",
    "airline_companies = [\"air france\", \"american airlines\", \"british airways\", \"delta airlines\", \"southwest airlines\", \"united airlines\", \"us airways\", \"virgin america\"]\n",
    "    \n",
    "# For each company, a dictionary composed of terms-counts is stored \n",
    "terms_by_company = {}\n",
    "for company in airline_companies:\n",
    "    terms_by_company[company] = {}\n",
    "\n",
    "pipeline = PreprocessingPipeline(True, True, False)\n",
    "\n",
    "for tweet in tweets:\n",
    "    \n",
    "    airlines = detect_airline(tweet)\n",
    "    \n",
    "    # If at least one company is detected\n",
    "    if len(airlines) > 0:\n",
    "        \n",
    "        sentiment = extract_sentiment(bestClassifier, tweet, bestBowObj)\n",
    "        \n",
    "        # We are not interested in neutral tweets\n",
    "        if sentiment != 1:\n",
    "            \n",
    "            # All the terms in the tweet\n",
    "            terms = []\n",
    "            \n",
    "            tokens = pipeline.preprocess(tweet)\n",
    "            bigrams = bigram(tokens)\n",
    "            trigrams = trigram(tokens)\n",
    "            \n",
    "            # Unigram : adjective or noun\n",
    "            unigram_tags = nltk.pos_tag(tokens, tagset='universal')\n",
    "            for tag in unigram_tags:\n",
    "                if tag[1] == \"ADJ\" or tag[1] == \"NOUN\":\n",
    "                    terms.append(tag[0])\n",
    "            \n",
    "            # Bigram : adjective-noun or noun-noun\n",
    "            for bi in bigrams:\n",
    "                decomposed = bi.split()\n",
    "                decomposed_tags = nltk.pos_tag(decomposed, tagset='universal')\n",
    "                if (decomposed_tags[0][1] == \"ADJ\" and decomposed_tags[1][1] == \"NOUN\") or (decomposed_tags[0][1] == \"NOUN\" and decomposed_tags[1][1] == \"NOUN\"):\n",
    "                    terms.append(bi)\n",
    "            \n",
    "            # Trigram : adjective-adjective-noun\n",
    "            for tri in trigrams:\n",
    "                decomposed = tri.split()\n",
    "                decomposed_tags = nltk.pos_tag(decomposed, tagset='universal')\n",
    "                if decomposed_tags[0][1] == \"ADJ\" and decomposed_tags[1][1] == \"ADJ\" and decomposed_tags[2][1] == \"NOUN\":\n",
    "                    terms.append(tri)\n",
    "            \n",
    "            # Updating the counts for each company detected in the tweet\n",
    "            for company in airlines:\n",
    "                for term in terms:\n",
    "                    # We don't take the name of the company itself\n",
    "                    if term.lower() not in company:\n",
    "                        if term in terms_by_company[company]:\n",
    "                            terms_by_company[company][term] += 1\n",
    "                        else:\n",
    "                            terms_by_company[company][term] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       word1  frequency1       word2  frequency2  \\\n",
      "air france               sad    0.044807      flight    0.036320   \n",
      "american airlines     flight    0.027778        time    0.020833   \n",
      "british airways      Awesome    0.033333         guy    0.033333   \n",
      "delta airlines      samantha    0.100000       james    0.100000   \n",
      "southwest airlines   Boardin    0.013889      flight    0.013889   \n",
      "united airlines       beauty    0.019108       today    0.019108   \n",
      "us airways            flight    0.035398  experience    0.011799   \n",
      "virgin america        flight    0.041237       board    0.020619   \n",
      "\n",
      "                            word3  frequency3     word4  frequency4  \\\n",
      "air france          France flight    0.027495     plane    0.026137   \n",
      "american airlines            quot    0.013889      bags    0.013889   \n",
      "british airways               bag    0.033333  security    0.033333   \n",
      "delta airlines                 Im    0.100000      cool    0.100000   \n",
      "southwest airlines         Dulles    0.013889     Vegas    0.013889   \n",
      "united airlines         Outsource    0.019108      fare    0.012739   \n",
      "us airways                   time    0.011799     Thank    0.008850   \n",
      "virgin america                 SD    0.010309       Fun    0.010309   \n",
      "\n",
      "                         word5  frequency5         word6  frequency6  \\\n",
      "air france            families    0.021385  France plane    0.015954   \n",
      "american airlines      project    0.013889          last    0.013889   \n",
      "british airways     EVERYTHING    0.033333      rucksack    0.033333   \n",
      "delta airlines           tunes    0.100000          coms    0.100000   \n",
      "southwest airlines      Orange    0.013889        County    0.013889   \n",
      "united airlines         people    0.012739          look    0.012739   \n",
      "us airways                  AA    0.008850          help    0.008850   \n",
      "virgin america           times    0.010309          safe    0.010309   \n",
      "\n",
      "                          word7  frequency7            word8  frequency8  \\\n",
      "air france               people    0.014257       passengers    0.012899   \n",
      "american airlines            PE    0.006944               AA    0.006944   \n",
      "british airways     Airways guy    0.033333  swab EVERYTHING    0.033333   \n",
      "delta airlines              nyc    0.100000   samantha james    0.100000   \n",
      "southwest airlines      excited    0.013889           United    0.013889   \n",
      "united airlines            http    0.012739       experience    0.012739   \n",
      "us airways                today    0.008850         customer    0.008850   \n",
      "virgin america              hat    0.010309              new    0.010309   \n",
      "\n",
      "                      word9  frequency9      word10  frequency10  \n",
      "air france            heart    0.011541       crash     0.009504  \n",
      "american airlines      toss    0.006944      mother     0.006944  \n",
      "british airways       staff    0.033333        free     0.033333  \n",
      "delta airlines      n delta    0.100000  cool tunes     0.100000  \n",
      "southwest airlines     next    0.013889        week     0.013889  \n",
      "united airlines        time    0.012739     Flights     0.012739  \n",
      "us airways            NEVER    0.008850     flights     0.008850  \n",
      "virgin america       avatar    0.010309   Fun times     0.010309  \n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Each company is associated to a list of the top 10 words with the 10 frequecies associated\n",
    "table = {}\n",
    "\n",
    "# Titles of the table rows\n",
    "index_companies = []\n",
    "\n",
    "for company in terms_by_company:\n",
    "    \n",
    "    index_companies.append(company)\n",
    "    \n",
    "    # Calculating the frequencies\n",
    "    counts = Counter(terms_by_company[company])\n",
    "    normalized_counts = {term : count / sum(counts.values()) for term, count in counts.items()} #\n",
    "    top10 = Counter(normalized_counts).most_common(10)\n",
    "    \n",
    "    # List : [word, frequeny, word, frequency, ...]\n",
    "    list_top10 = []\n",
    "    for count in top10:\n",
    "        list_top10.append(count[0])\n",
    "        list_top10.append(count[1])\n",
    "        \n",
    "    table[company] = list_top10\n",
    "\n",
    "# 10 columns for the words and 10 columns for the frequencies\n",
    "columnNames = []\n",
    "for i in range(1, 11):\n",
    "    columnNames.append(\"word\" + str(i))\n",
    "    columnNames.append(\"frequency\" + str(i))\n",
    "\n",
    "# Constructing the table\n",
    "df = pd.DataFrame.from_dict(table, orient='index', columns=columnNames)\n",
    "print(df)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.2 \n",
    "\n",
    "The table generated in the Question 12 can lead us to any conclusion about each one of the 9 companies? Can we identify specific events that have occured during the data retrieval?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 \n",
    "\n",
    "Person names, companies names and locations are called named entities. Named-entity recognition (NER) is the task of extracting named entities  classifying them using pre-defined categories. In this bonus section, you will use a Named Entity Recognizer to automatically extract named entities from the tweets. This approach is generic enough to retrieve information about other companies or even product and people names.\n",
    "\n",
    "you have to use the tweet database of the previous section (https://drive.google.com/file/d/1Cuw6Y12Bj91vF_iH49mqPZZfJkY92iBY/view?usp=sharing)**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 \n",
    "\n",
    "Implement a code that generates the table with the top 10 most mentioned named entities in the database (this table has to contain the frequencies of the name entities). After that, generates a bar chart that despicts the number of positive, negative and neutral tweets for each one of these 10 named entities. Briefly describe the results found in the bar chart.\n",
    "\n",
    "*Ignore the named entities related to the following airline companies : Air France, American, British Airways,  Delta, Southwest, United, Us Airways and Virgin America.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tag import StanfordNERTagger\n",
    "import os\n",
    "java_path = \"C:/Program Files (x86)/Java/jdk1.8.0_161/bin/java.exe\"\n",
    "os.environ['JAVAHOME'] = java_path\n",
    "\n",
    "#tweets = extract_tweet_content(\"e_corp_dataset.txt\")\n",
    "#tweets = extract_tweet_content(\"test.txt\")\n",
    "\n",
    "airline_companies = [\"air france\", \"american airlines\", \"british airways\", \"delta airlines\", \"southwest airlines\", \"united airlines\", \"us airways\", \"virgin america\"]\n",
    "    \n",
    "# Each entity associated with the number of occurences\n",
    "all_entities = {}\n",
    "\n",
    "pipeline = PreprocessingPipeline(True, True, False)\n",
    "\n",
    "st = StanfordNERTagger('stanford-ner-2018-10-16/classifiers/english.all.3class.distsim.crf.ser.gz',\n",
    "                           'stanford-ner-2018-10-16/stanford-ner.jar', encoding='utf-8')\n",
    "\n",
    "for tweet in tweets:\n",
    "    \n",
    "    tokens = pipeline.preprocess(tweet)\n",
    "    \n",
    "    # Get the classification of each token (list of tuples)\n",
    "    classified = st.tag(tokens)\n",
    "    \n",
    "    for word in classified:\n",
    "        # Classified as \"O\" means that it is NOT an entity\n",
    "        # We don't take the name of the company itself\n",
    "        if word[1] != \"O\" and word[0].lower() not in airline_companies:\n",
    "            if word[0] not in all_entities:\n",
    "                all_entities[word[0]] = 1\n",
    "            else:\n",
    "                all_entities[word[0]] += 1\n",
    "\n",
    "# Calculating the frequencies\n",
    "counts = Counter(all_entities)\n",
    "normalized_counts = {entity : count / sum(counts.values()) for entity, count in counts.items()} #\n",
    "\n",
    "# Retrieving the 10 most common entities and their frequency\n",
    "top10 = Counter(normalized_counts).most_common(10)\n",
    "\n",
    "top10_names = []\n",
    "table10 = []\n",
    "for entity in top10:\n",
    "    table10.append(list(entity))\n",
    "    top10_names.append(entity[0])\n",
    "\n",
    "# Table of 2 columns containing the top 10 entities and their frequency\n",
    "df = pd.DataFrame(table10, columns = ['entity', 'frequency'])\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same as detect_airline but takes the words to detect as input\n",
    "def detect_entity(tweet, to_detect):\n",
    "    detected = []\n",
    "\n",
    "    for entity in to_detect:\n",
    "        if entity in tweet:\n",
    "            detected.append(entity)\n",
    "    \n",
    "    return detected\n",
    "\n",
    "# Each entity is associated to a triplet (counts of neg, neut, pos)\n",
    "counts = {}\n",
    "for entity in top10_names:\n",
    "    counts[entity] = [0, 0, 0]\n",
    "    \n",
    "for tweet in tweets:\n",
    "    entities = detect_entity(tweet, top10_names)\n",
    "    if len(entities) > 0:\n",
    "        sentiment = extract_sentiment(bestClassifier, tweet, bestBowObj)\n",
    "        for entity in entities:\n",
    "            counts[entity][sentiment] += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "X = []\n",
    "negative = []\n",
    "neutral = []\n",
    "positive = []\n",
    "\n",
    "for key, value in counts.items():\n",
    "    X.append(key)\n",
    "    negative.append(value[0])\n",
    "    neutral.append(value[1])\n",
    "    positive.append(value[2])\n",
    "\n",
    "# Constructing the bar chart for the top 10 entities\n",
    "df = pd.DataFrame(np.c_[negative, neutral, positive], index=X)\n",
    "df.plot.bar(color = [\"red\", \"grey\", \"green\"])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 \n",
    "\n",
    "Generate a similar table produced in the steps 12 for the 10 most mentioned named entities. Can we draw any conclusion about these named entities?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAME CODE AS QUESTION 12 BUT USING THE 10 MOST MENTIONED ENTITIES INSTEAD OF AIRLINES\n",
    "\n",
    "tweets = extract_tweet_content(\"e_corp_dataset.txt\")\n",
    "#tweets = extract_tweet_content(\"test.txt\")\n",
    "\n",
    "terms_by_entity = {}\n",
    "for entity in top10_names:\n",
    "    terms_by_entity[entity] = {}\n",
    "\n",
    "pipeline = PreprocessingPipeline(True, True, True)\n",
    "\n",
    "for tweet in tweets:\n",
    "    entities = detect_entity(tweet, top10_names)\n",
    "    if len(entities) > 0:\n",
    "        sentiment = extract_sentiment(bestClassifier, tweet, bestBowObj)\n",
    "        if sentiment != 1:\n",
    "            terms = []\n",
    "            \n",
    "            tokens = pipeline.preprocess(tweet)\n",
    "            \n",
    "            bigrams = bigram(tokens)\n",
    "            trigrams = trigram(tokens)\n",
    "            \n",
    "            unigram_tags = nltk.pos_tag(tokens, tagset='universal')\n",
    "            for tag in unigram_tags:\n",
    "                if tag[1] == \"ADJ\" or tag[1] == \"NOUN\":\n",
    "                    terms.append(tag[0])\n",
    "            \n",
    "            for bi in bigrams:\n",
    "                decomposed = bi.split()\n",
    "                decomposed_tags = nltk.pos_tag(decomposed, tagset='universal')\n",
    "                if (decomposed_tags[0][1] == \"ADJ\" and decomposed_tags[1][1] == \"NOUN\") or (decomposed_tags[0][1] == \"NOUN\" and decomposed_tags[1][1] == \"NOUN\"):\n",
    "                    terms.append(bi)\n",
    "            \n",
    "            for tri in trigrams:\n",
    "                decomposed = tri.split()\n",
    "                decomposed_tags = nltk.pos_tag(decomposed, tagset='universal')\n",
    "                if decomposed_tags[0][1] == \"ADJ\" and decomposed_tags[1][1] == \"ADJ\" and decomposed_tags[2][1] == \"NOUN\":\n",
    "                    terms.append(tri)\n",
    "                    \n",
    "            for entity in entities:\n",
    "                for term in terms:\n",
    "                    if term not in entity:\n",
    "                        if term in terms_by_entity[entity]:\n",
    "                            terms_by_entity[entity][term] += 1\n",
    "                        else:\n",
    "                            terms_by_entity[entity][term] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "table = {}\n",
    "index_entities = []\n",
    "for entity in terms_by_entity:\n",
    "    index_companies.append(company)\n",
    "    counts = Counter(terms_by_entity[entity])\n",
    "    normalized_counts = {term : count / sum(counts.values()) for term, count in counts.items()} #\n",
    "    top10 = Counter(normalized_counts).most_common(10)\n",
    "    list_top10 = []\n",
    "    for count in top10:\n",
    "        list_top10.append(count[0])\n",
    "        list_top10.append(count[1])\n",
    "    if len(list_top10) > 0:\n",
    "        table[entity] = list_top10\n",
    "\n",
    "columnNames = []\n",
    "for i in range(1, 11):\n",
    "    columnNames.append(\"word\" + str(i))\n",
    "    columnNames.append(\"frequency\" + str(i))\n",
    "    \n",
    "df = pd.DataFrame.from_dict(table, orient='index', columns=columnNames)\n",
    "print(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
